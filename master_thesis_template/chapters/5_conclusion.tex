%!TEX root = ../master_thesis.tex
\chapter{Conclusion and Future Research}
\label{ch:conclusion}
In this thesis, we introduced a generative adversarial network based model for unsupervised cross-domain speech-to-speech synthesis. We demonstrated the performance of our model on the task of translating the voice of male speakers to the voice of female speakers and vice versa. Our model utilizes the time-frequency representation of speech signals. The time-frequency representation of speech signals is obtained using a short-time Fourier transform (STFT) operation. In contrast to time-amplitude representation, the time-frequency representation is robust for the generation of high-quality speech signals. However, the choice of the time-frequency representation poses the challenge of the spectrogram consistency, that is finding a coherent time-domain waveform, which is the target for the task of speech synthesis. We outlined a comprehensive discussion on the consistency of the time-frequency representation. We presented two experimental setups to address the problem of coherent waveform reconstruction. In the first setup, we ignore the phase spectrogram and only utilize the magnitude spectrogram as an input to our model. Furthermore, we utilize the consistency condition of the magnitude-spectrogram to define a loss term to constrain the model such that translated spectrograms (male-to-female or female-to-male) have a valid time-domain waveform. During inference time we use the Griffin-Lim algorithm to reconstruct a coherent time-domain waveform from the translated magnitude-spectrogram, thus, accomplishing our goal of speech-to-speech synthesis. In the second setup, we utilize the full time-frequency representation as an input to our model. We transform the phase spectrogram to the instantaneous frequency (IF) representation. The IF is free of discontinuities present in the phase spectrogram which we hypothesize should make it easier for the model to learn meaningful features. In this setup, we utilize the consistency condition of the full time-frequency representation by performing an iteration of the STFT and inverse-STFT (ISTFT) operation on the translated spectrogram. Finally, at inference time to perform speech synthesis we apply the ISTFT operation on the translated spectrograms.

We performed the quantitative as well as a qualitative evaluation of the different models. We report three quantitative scores, namely the inception score (IS), the Fr\'echet inception distance (FID) and the domain classification accuracy which capture different facets of the model performance. We further show that the FID is a more reliable score since it is robust to different kinds of noise disturbances relating to the perceptual nature of sound. We performed extensive experiments based on different scales of the spectrogram representation. We showed the log scaling performs best in terms of the quality of the speech waveform. In the first setup, our empirical results demonstrate the importance of the spectrogram consistency constraint. For our consistency based model (VC-Con-GAN), we achieved lower a FID and the best MOS compared across all models. Furthermore, we performed a significance test and show the consistency based model is a clear winner. The results of our second setup are better in terms of the FID score but are not good in terms of the perceptual quality of speech. We think this could be improved by finding the right combination of hyper-parameters of the consistency loss term. Due to the limited time and high computational costs, we leave it as the future work.

Our work offers several promising research directions. In future work, we would like to investigate the methods for the latent space disentanglement of an input speech spectrogram. Specifically, we would like to learn two unique representations of an input speech signal. One representation to encode domain-specific features like phonetics, tone variations of speakers, and the other representation to encode domain-independent information like the linguistic features. By disentangling, the joint space into a domain-specific and a domain-independent space we can precondition the generation process on the specific speaker identifier. This will allow us to translate the voice of an input speaker to the desired target speaker. Such an application can be used to investigate the robustness of automatic-speech-recognition (ASR) systems. It can be used to demonstrate the threat to an ASR system in form of adversarial fake samples generated by the model. Besides this, our work can be used to introduce voice privacy in the online interview screening process. This will prevent discrimination based on the gender or race of an interviewee. The interviewee could select a required domain and a speaker identifier to convert their voice, and the interviewer will solely make the decision based on the performance.

Finally, we hope our work will increase the possibilities of time-frequency based GAN models for the adversarial generation of speech signals. Furthermore, we are optimistic that the work will open up the application to domain translation tasks for other audio signals, for instance in music genres translating from guitar to violin and many more.  
