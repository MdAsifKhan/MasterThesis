%!TEX root = ../master_thesis.tex
% \abstract
\chapter*{\centering \Large Abstract}
\label{abstract}
\addcontentsline{toc}{chapter}{Abstract}
%A key problem encountered in developing a machine learning (ML) system is its robustness to the situation at test time which was limited by the data during training. 
%One of the requirements for a model to generalize well at the test time is the large number of diverse samples at the training time. One of the requirements for robustness is the diversity of samples at the training time. The diversity can be obtained by data augmentation. Domain translation is an approach to translate data from the source domain to the target domain. 
The recent advancement in generative adversarial networks provides an end-to-end framework for translating data across domains. 
%The translated data can later be used to augment the learning process and development of robust ML systems.The unsupervised image-to-image translation proposed a concept of shared latent space and cycle consistency loss to translate images from one domain to another when paired data is not available. 
The idea has been extensively explored for style transfer in images with an application like translating a photograph to the ``Monet Impression, Sunrise". In this work, we build on this idea and extend it to the task of speech-to-speech synthesis. In our setup, we define speech-to-speech synthesis as the task of translating the voice of the male speaker to that of the female speaker and vice versa. Our work offers several interesting applications. For instance, it can be used to generate artificial data to augment and improve automatic speech recognition systems, or it can be used to achieve better voice privacy in online gaming platforms.  

In this thesis, we work with the time-frequency (TF) representation of speech signals. 
%The time-frequency representation consists of two parts: magnitude spectrogram and phase spectrogram. 
The speech signals are highly periodic and have irregularities and discontinuities. Thus, to obtain the stable TF representation, short-time Fourier transform (STFT) is performed with the overlapping time frames. %As a consequence, we end up with the over-complete representation. 
A key hurdle in using this representation for training neural networks is that the chain of non-linear transformations ignores the overlap factor. As a consequence, the generated TF representations can be unstable which makes it difficult to find their corresponding time-domain waveforms to perform speech synthesis. In such a scenario, the reconstruction of the time domain waveform relies on the consistency\footnote{The time-frequency representation is said to be consistent if it can be obtained by taking the STFT of any time-domain waveform.} of the TF representation.
%In our work, we exploit the consistency condition of a TF representation to constrain the learning of our model so that it generates consistent TF representations.  
%This will further allow us to find their time domain waveform using inverse STFT operation.

The TF representation consists of two parts: magnitude and phase spectrogram. We propose two experimental setups: (i) Due to convoluted nature of phase spectrogram, we only use the magnitude part for the training of our model and to perform synthesis at inference time, we reconstruct time-domain speech waveform using the Griffin-Lim algorithm (GLA). In this case, we use the consistency condition of the magnitude spectrogram to constrain our model. (ii) We use the full TF representation. We transform the phase spectrogram to the instantaneous frequency representation which also relates to perceptual quality of sound. In this case, we exploit the consistency condition of the complete TF representation and to perform synthesis at inference we avoid use of GLA and reconstruct time-domain speech waveform using inverse-STFT operation. Our results demonstrate that the consistency constrained model provides a perceptually better quality of speech-to-speech synthesis.

%A common approach is to use the magnitude representation and reconstruct the phase information using iterative methods like Griffin-Lim algorithm. Such iterative methods find a phase representation to reconstruct a closest time domain signal. However, finding a coherent reconstruction is an ill-posed problem and is only guaranteed if the spectrogram is consistent. In our work, we present two setups: (i) We use the magnitude representation and reconstruct the phase information using the Griffin-Lim algorithm. We use the consistency condition of a spectrogram to define a loss term which constrains the neural network model to generate a consistent spectrogram. (ii) We use the full spectrogram representation. To deal with discontinuities in the phase spectrogram we transform it to the instantaneous frequency (IF) representation. Furthermore, for the consistency of the spectrogram, we propose a projection loss term inspired by the iterative operation of the Griffin-Lim algorithm.


  